{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca78d7e7",
   "metadata": {},
   "source": [
    "# GMA Insurance – Claims Cost Prediction Project  \n",
    "**AMDARI(10NALYTICS INTERNSHIP PROJECT)**  \n",
    "**Consultant:** Patience Akwara\n",
    "\n",
    "# 03 — Data Cleaning & Merge\n",
    "\n",
    "## Objectives\n",
    "- Standardize numerical, date, and categorical data  \n",
    "- Remove rows with impossible or contradictory values  \n",
    "- Create a clean, merged dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8244b-f7f5-4e85-beb2-40e02a69afca",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36523181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 13), (5000, 11), (2410, 4))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# creating base directory to store loaded files\n",
    "base = Path('.')\n",
    "claims = pd.read_csv(base/'claims.csv')\n",
    "policyholders = pd.read_csv(base/'policyholders.csv')\n",
    "third_parties = pd.read_csv(base/'third_parties.csv')\n",
    "\n",
    "claims.shape, policyholders.shape, third_parties.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe73ad-0b18-4f8f-8945-e248e1439bfc",
   "metadata": {},
   "source": [
    "## _Insights:_ \n",
    "\n",
    "**All three datasets were successfully loaded and are structurally consistent for downstream preparation. The Claims table contains 8,000 records across 13 fields, representing the core claim-level observation set. The Policyholders table has 5,000 records and 11 fields, indicating that multiple claims can be associated with a single policyholder/policy (many-to-one relationship). The Third Parties table contains 2,410 records and 4 fields, suggesting third-party involvement is present for a subset of claims and should be treated as an exposure feature via claim-level aggregation (e.g., third-party presence and count). From an actuarial frequency–severity perspective, these dimensions support modelling of both claim incidence/exposure (frequency proxies) and cost drivers (severity proxies) once merged on the appropriate keys.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e85907d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimming whitespace and normalising spacing \n",
    "def standardize_str(s: pd.Series) -> pd.Series:\n",
    "    return s.astype('string').str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "def to_datetime_cols(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    for c in cols:\n",
    "        df[c] = pd.to_datetime(df[c], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def winsorize_series(x: pd.Series, lower_q=0.01, upper_q=0.99) -> pd.Series:\n",
    "    x_nonnull = x.dropna()\n",
    "    if x_nonnull.empty:\n",
    "        return x\n",
    "    lo, hi = x_nonnull.quantile([lower_q, upper_q]).values\n",
    "    return x.clip(lower=lo, upper=hi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcee832-b819-4ead-97db-7c1f11826e4a",
   "metadata": {},
   "source": [
    "## _Insights:_\n",
    "**These helper functions establish a consistent, auditable preprocessing foundation for modelling. standardize_str() enforces uniform categorical formatting by trimming whitespace and normalising spacing, reducing the risk of fragmented categories and improving encoding stability. to_datetime_cols() standardises all date fields into a common datetime type with safe coercion, enabling reliable construction of temporal features (e.g., reporting and settlement lags) that are central to reserving workflows. winsorize_series() provides a controlled method for limiting the influence of extreme values by capping observations at selected quantiles; actuarially, this supports severity modelling by stabilising variance while retaining the economic signal of high-loss claims, making subsequent estimation and model training more robust.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43230a62-15b0-43ef-a046-9019b793bc4e",
   "metadata": {},
   "source": [
    "## Date Standardisation & Lag Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589d664b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Date</th>\n",
       "      <th>FNOL_Date</th>\n",
       "      <th>Settlement_Date</th>\n",
       "      <th>Days_To_FNOL</th>\n",
       "      <th>Days_To_Settlement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2019-03-23</td>\n",
       "      <td>1</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-19</td>\n",
       "      <td>2021-10-19</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>0</td>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-18</td>\n",
       "      <td>2021-06-18</td>\n",
       "      <td>2021-09-13</td>\n",
       "      <td>0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-21</td>\n",
       "      <td>2021-03-24</td>\n",
       "      <td>2021-05-26</td>\n",
       "      <td>3</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Accident_Date  FNOL_Date Settlement_Date  Days_To_FNOL  Days_To_Settlement\n",
       "0    2019-12-19 2019-12-19      2020-03-01             0                73.0\n",
       "1    2018-12-30 2018-12-31      2019-03-23             1                82.0\n",
       "2    2021-10-19 2021-10-19      2022-04-22             0               185.0\n",
       "3    2021-06-18 2021-06-18      2021-09-13             0                87.0\n",
       "4    2021-03-21 2021-03-24      2021-05-26             3                63.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Standardize strings and parse dates\n",
    "claims_clean = claims.copy()\n",
    "policyholders_clean = policyholders.copy()\n",
    "third_parties_clean = third_parties.copy()\n",
    "\n",
    "for df in (claims_clean, policyholders_clean, third_parties_clean):\n",
    "    for c in [c for c in df.columns if df[c].dtype == 'object']:\n",
    "        df[c] = standardize_str(df[c])\n",
    "\n",
    "claims_date_cols = [c for c in claims_clean.columns if c.lower().endswith('date')]\n",
    "claims_clean = to_datetime_cols(claims_clean, claims_date_cols)\n",
    "\n",
    "if {'Accident_Date','FNOL_Date'}.issubset(claims_clean.columns):\n",
    "    claims_clean['Days_To_FNOL'] = (claims_clean['FNOL_Date'] - claims_clean['Accident_Date']).dt.days\n",
    "if {'FNOL_Date','Settlement_Date'}.issubset(claims_clean.columns):\n",
    "    claims_clean['Days_To_Settlement'] = (claims_clean['Settlement_Date'] - claims_clean['FNOL_Date']).dt.days\n",
    "\n",
    "claims_clean[['Accident_Date','FNOL_Date','Settlement_Date','Days_To_FNOL','Days_To_Settlement']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb265a-aa91-41a4-9588-c23e3e822263",
   "metadata": {},
   "source": [
    "## _Insights:_\n",
    "**This step standardises all date fields into a consistent datetime format and derives two actuarially meaningful timeline features: Days_To_FNOL (reporting delay) and Days_To_Settlement (settlement duration). The results shown are operationally plausible—FNOL delays are typically very short (often same-day or next-day), while settlement durations span weeks to months, reflecting claim handling complexity and severity. From a reserving and frequency–severity perspective, these lag variables act as early indicators of claim development: longer settlement durations are commonly associated with higher uncertainty, higher case reserves, and potentially greater ultimate severity, while reporting delays may correlate with claim type, fraud propensity, or documentation complexity. These engineered features therefore strengthen modelling by capturing claim lifecycle dynamics rather than relying only on static FNOL attributes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd65652-e3ff-419b-b816-5bbdc4e94b4c",
   "metadata": {},
   "source": [
    "## Third-Party Severity Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b10e7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TP_Injury_Severity_code']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Standardize categorical severity levels (Minor/Serious/Fatal -> 1/2/3) where present\n",
    "sev_map = {'minor': 1, 'serious': 2, 'fatal': 3}\n",
    "\n",
    "def map_minor_serious_fatal(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype('string').str.lower().str.strip()\n",
    "    return s.map(sev_map)\n",
    "\n",
    "for c in third_parties_clean.columns:\n",
    "    if str(third_parties_clean[c].dtype) in ('object','string'):\n",
    "        vals = set(third_parties_clean[c].dropna().astype(str).str.lower().str.strip().unique().tolist())\n",
    "        if vals and vals.issubset(set(sev_map.keys())):\n",
    "            third_parties_clean[c + '_code'] = map_minor_serious_fatal(third_parties_clean[c])\n",
    "\n",
    "[c for c in third_parties_clean.columns if c.endswith('_code')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960e8d0-0032-404c-9671-70453c3cd67f",
   "metadata": {},
   "source": [
    "## _Insights:_\n",
    "**This step converts third-party injury severity categories (Minor, Serious, Fatal) into an ordinal numeric scale (1, 2, 3) and successfully identifies the relevant field, creating TP_Injury_Severity_code. The ordinal encoding preserves the inherent ranking of medical severity, which is actuarially important because severity levels typically correspond to increasing claim costs, longer settlement durations, and greater reserve uncertainty. From a frequency–severity perspective, this feature supports modelling the severity component of third-party exposure by translating qualitative injury descriptors into a consistent, model-ready form while reducing category inconsistency risk across records.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578ab9c-b0b6-41dd-b21b-c4079b31c975",
   "metadata": {},
   "source": [
    "## Handling Duplicates & Timeline Contradictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a53a95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, (8000, 15))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Remove duplicates and contradictory timeline records\n",
    "dup_before = len(claims_clean) - claims_clean['Claim_ID'].nunique()\n",
    "claims_clean = claims_clean.drop_duplicates(subset=['Claim_ID'], keep='first')\n",
    "\n",
    "rules_mask = pd.Series(False, index=claims_clean.index)\n",
    "if 'Days_To_FNOL' in claims_clean.columns:\n",
    "    rules_mask |= claims_clean['Days_To_FNOL'].notna() & (claims_clean['Days_To_FNOL'] < 0)\n",
    "if 'Days_To_Settlement' in claims_clean.columns:\n",
    "    rules_mask |= claims_clean['Days_To_Settlement'].notna() & (claims_clean['Days_To_Settlement'] < 0)\n",
    "if {'Status','Settlement_Date'}.issubset(claims_clean.columns):\n",
    "    rules_mask |= (claims_clean['Status'].str.lower() == 'settled') & claims_clean['Settlement_Date'].isna()\n",
    "\n",
    "removed_contradictory = int(rules_mask.sum())\n",
    "claims_clean = claims_clean.loc[~rules_mask].copy()\n",
    "\n",
    "dup_before, removed_contradictory, claims_clean.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac19382-08b2-44cb-98c5-76d05435c3d4",
   "metadata": {},
   "source": [
    "## _Insights:_\n",
    "**This validation step confirms strong structural data quality. No duplicate records were detected at the Claim_ID level (dup_before = 0), indicating the claims table is already uniquely keyed and suitable for downstream merges. In addition, no contradictory timeline records were identified (removed_contradictory = 0): there are no negative reporting lags (FNOL before accident), no negative settlement lags (settlement before FNOL), and no cases marked “settled” with a missing settlement date. From an actuarial perspective, this is important because it suggests the claim lifecycle dates are internally coherent, supporting credible development analysis (reporting/settlement patterns) and reducing the risk of distortion in reserving or severity modelling driven by data errors rather than true loss behaviour.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b3856-3151-4796-9305-5dbe95589f68",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a556ccc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimated_Claim_Amount</th>\n",
       "      <th>Ultimate_Claim_Amount</th>\n",
       "      <th>Estimated_Claim_Amount_winsor</th>\n",
       "      <th>Estimated_Claim_Amount_log1p</th>\n",
       "      <th>Ultimate_Claim_Amount_winsor</th>\n",
       "      <th>Ultimate_Claim_Amount_log1p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5243</td>\n",
       "      <td>2808.0</td>\n",
       "      <td>5243.0</td>\n",
       "      <td>8.564840</td>\n",
       "      <td>2808.0</td>\n",
       "      <td>7.940584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3934</td>\n",
       "      <td>2952.0</td>\n",
       "      <td>3934.0</td>\n",
       "      <td>8.277666</td>\n",
       "      <td>2952.0</td>\n",
       "      <td>7.990577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153631</td>\n",
       "      <td>156497.0</td>\n",
       "      <td>153631.0</td>\n",
       "      <td>11.942315</td>\n",
       "      <td>156497.0</td>\n",
       "      <td>11.960799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2812</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>2812.0</td>\n",
       "      <td>7.942007</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>7.280008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5094</td>\n",
       "      <td>4243.0</td>\n",
       "      <td>5094.0</td>\n",
       "      <td>8.536015</td>\n",
       "      <td>4243.0</td>\n",
       "      <td>8.353261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Estimated_Claim_Amount  Ultimate_Claim_Amount  \\\n",
       "0                    5243                 2808.0   \n",
       "1                    3934                 2952.0   \n",
       "2                  153631               156497.0   \n",
       "3                    2812                 1450.0   \n",
       "4                    5094                 4243.0   \n",
       "\n",
       "   Estimated_Claim_Amount_winsor  Estimated_Claim_Amount_log1p  \\\n",
       "0                         5243.0                      8.564840   \n",
       "1                         3934.0                      8.277666   \n",
       "2                       153631.0                     11.942315   \n",
       "3                         2812.0                      7.942007   \n",
       "4                         5094.0                      8.536015   \n",
       "\n",
       "   Ultimate_Claim_Amount_winsor  Ultimate_Claim_Amount_log1p  \n",
       "0                        2808.0                     7.940584  \n",
       "1                        2952.0                     7.990577  \n",
       "2                      156497.0                    11.960799  \n",
       "3                        1450.0                     7.280008  \n",
       "4                        4243.0                     8.353261  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Handle outliers: winsorization + log transforms\n",
    "money_cols = [c for c in ['Estimated_Claim_Amount','Ultimate_Claim_Amount'] if c in claims_clean.columns]\n",
    "for c in money_cols:\n",
    "    claims_clean[c] = pd.to_numeric(claims_clean[c], errors='coerce')\n",
    "    claims_clean[c + '_winsor'] = winsorize_series(claims_clean[c], 0.01, 0.99)\n",
    "    claims_clean[c + '_log1p'] = np.log1p(claims_clean[c + '_winsor'])\n",
    "\n",
    "claims_clean[[c for c in claims_clean.columns if 'Amount' in c]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7257c-2e14-41fa-a77c-e2b216a5d1ca",
   "metadata": {},
   "source": [
    "## _Insights:_\n",
    "**This step applies a two-stage approach to stabilise the heavy-tailed nature of claim severities while retaining actuarial signal. First, winsorization (1st–99th percentiles) caps extreme monetary values in Estimated_Claim_Amount and Ultimate_Claim_Amount, reducing the undue influence of rare, very large losses on model fitting without deleting observations. Second, the log1p transformation converts the capped amounts to a scale that is closer to normality and more homoscedastic, which improves statistical efficiency and predictive stability. From a frequency–severity reserving perspective, this approach supports more robust severity modelling by controlling tail-driven volatility while still preserving the economic relevance of large-loss claims in the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf1306-17de-494f-a1b6-f0adf4f3fded",
   "metadata": {},
   "source": [
    "## Dataset Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29277130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 33),\n",
       "    tp_count  has_third_party\n",
       " 0         1                1\n",
       " 1         0                0\n",
       " 2         1                1\n",
       " 3         0                0\n",
       " 4         0                0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Merge tables using primary keys + aggregate third parties to claim level\n",
    "merged = claims_clean.merge(policyholders_clean, on='Policy_ID', how='left', suffixes=('', '_ph'))\n",
    "\n",
    "tp = third_parties_clean.copy()\n",
    "tp['has_third_party'] = 1\n",
    "\n",
    "num_cols = [c for c in tp.columns if pd.api.types.is_numeric_dtype(tp[c]) and c not in ['has_third_party']]\n",
    "named_aggs = {\n",
    "    'tp_count': ('Claim_ID', 'size'),\n",
    "    'has_third_party': ('has_third_party', 'max'),\n",
    "}\n",
    "for c in num_cols:\n",
    "    named_aggs[f'{c}_mean'] = (c, 'mean')\n",
    "    named_aggs[f'{c}_max'] = (c, 'max')\n",
    "\n",
    "tp_agg = tp.groupby('Claim_ID').agg(**named_aggs).reset_index()\n",
    "merged = merged.merge(tp_agg, on='Claim_ID', how='left')\n",
    "\n",
    "merged['tp_count'] = merged['tp_count'].fillna(0).astype(int)\n",
    "merged['has_third_party'] = merged['has_third_party'].fillna(0).astype(int)\n",
    "\n",
    "merged.shape, merged[['tp_count','has_third_party']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbefd16d-5115-4122-b97b-0c9de68d0ade",
   "metadata": {},
   "source": [
    "## _Insights:_\n",
    "**This step constructs a single modelling dataset by merging Claims with Policyholders on Policy_ID (many-to-one enrichment) and incorporating Third Party information via claim-level aggregation on Claim_ID (one-to-many compression). Aggregating third-party records into features such as tp_count and has_third_party preserves third-party exposure while preventing row duplication and inflated claim weights. From an actuarial frequency–severity perspective, these engineered variables act as frequency/exposure proxies (presence and count of third parties) and provide a structured mechanism to incorporate third-party-driven severity effects into ultimate cost modelling. The explicit zero-filling for missing third-party joins appropriately encodes “no third-party involvement” rather than treating it as missing data, improving interpretability and downstream model stability.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac7a52-b6e3-4625-8e46-6fbafb8632b9",
   "metadata": {},
   "source": [
    "## Modelling-Ready Export: Target Known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79ab9578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 7575, WindowsPath('cleaned_claims_dataset.csv'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Export modelling-ready dataset (target known)\n",
    "target_col = 'Ultimate_Claim_Amount'\n",
    "before = len(merged)\n",
    "merged_model = merged.loc[merged[target_col].notna()].copy()\n",
    "after = len(merged_model)\n",
    "\n",
    "out_path = base/'cleaned_claims_dataset.csv'\n",
    "merged_model.to_csv(out_path, index=False)\n",
    "\n",
    "before, after, out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0c47fe-d1f3-48c3-8447-bf3421df156f",
   "metadata": {},
   "source": [
    "## _Insights:_\n",
    "**This step creates a supervised learning dataset by filtering to claims with a non-missing Ultimate_Claim_Amount and exporting the cleaned, merged table. The row count decreases from 8,000 to 7,575, implying 425 claims (~5.3%) lack an observed ultimate cost—consistent with open/unsettled claims rather than data quality failure. From an actuarial reserving perspective, this separation is appropriate: closed claims with realised ultimate values are suitable for model calibration (severity estimation), while open claims should be retained for prospective prediction and monitoring, reflecting the natural claims development process and avoiding target leakage.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d818a0f",
   "metadata": {},
   "source": [
    "## Data Cleaning Summary \n",
    "\n",
    "- **Data integrity:** Duplicate claims removed using `Claim_ID` (duplicates detected: **0**).  \n",
    "- **Contradiction controls:** Records with negative reporting/settlement lags or inconsistent settlement indicators were removed (removed: **0**).  \n",
    "- **Lifecycle realism:** Open claims (missing `Ultimate_Claim_Amount`) were excluded from supervised modelling (rows before target filter: **8000**, after: **7575**).  \n",
    "- **Tail management:** Monetary fields were winsorized at the 1st/99th percentiles and log-transformed to stabilise variance while retaining economically meaningful extreme losses.  \n",
    "- **Exposure enrichment:** Third-party records were aggregated to claim level (e.g., `tp_count`, `has_third_party`) to jointly capture frequency (presence/count) and severity drivers.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
